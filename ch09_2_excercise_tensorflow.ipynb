{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the main benefits of creating a computation graph rather than directly executing the computations? What are the main drawbacks?\n",
    "Pros\n",
    "- It will be easier to visualize the workflow\n",
    "- It is easier to optimize each step and do parallel computation\n",
    "- tensor flow can automatically calculate gradient using reverse mode auto diff\n",
    "\n",
    "Cons\n",
    "- It is harder to debug and intractively do some operations\n",
    "- It is harder to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the statement a_val = a.eval(session=sess) equivalent to a_val = sess.run(a)?\n",
    "- yes both will evaluate operation \"a\" in context of session \"sess\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the statement a_val, b_val = a.eval(session=sess), b.eval(session=sess) equivalent to a_val, b_val = sess.run([a, b])?\n",
    "- No\n",
    "- The first estement will run the graph twice each to calcualte a and b \n",
    "- But the latter statement will compute both operations in single graph. Which has following effect\n",
    "    - If there is some side effect of a such as changing some variable then it may affect b\n",
    "    - If operation a and b both use some intermediate operation then they will reuse so second statement will be faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you run two graphs in the same session?\n",
    "- No, two graph must be merged to one to run on single session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you create a graph g containing a variable w, then start two threads and open a session in each thread, both using the same graph g, will each session have its own copy of the variable w or will it be shared?\n",
    "- yes, different session can run same graph, but each will have its own copy of variable\n",
    "- However in distributed tensor flow variables are stored in containers managed by cluster, so if two session connect to same container they will share the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When is a variable initialized? When is it destroyed?\n",
    "- variable is initialized when variable initializer or golbal variable initializer is run\n",
    "- variable is destroyed when session is closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between a placeholder and a variable?\n",
    "- Variable is used to store values that can be changed. Placeholder is used to feed values to an operation at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens when you run the graph to evaluate an operation that depends on a placeholder but you donâ€™t feed its value? What happens if the operation does not depend on the placeholder?\n",
    "- To evaluate operation that depends on the placeholder we need to feed value to placeholder using \"feed_dict\" paramter whose key should be the placeholder.\n",
    "- If value is not feed it will throw exception \n",
    "- If operation doesn't depends on placeholder its not needed to feed the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you run a graph, can you feed the output value of any operation, or just the value of placeholders?\n",
    "- When running graph output of any opertion can be feed this is usually used in case when output of freezed operation is cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can you set a variable to any value you want (during the execution phase)?\n",
    "- using assign operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?\n",
    "\n",
    "- reverse-mode autodiff need to traverse the graph only twice to find gradients with any number of variables\n",
    "- forward mode autodiff will need to traverse the graph one for each variable\n",
    "- Symbolic differentiation create separate graph to compute gradient. Highly optimized symbolic differentiation nees only one pass through the graph. But the graph may be very complex and inefficent than the original graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
