{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, One main aim of random initialization is breaking symmetry. If will be hard for back propagation to break the symmetry. All neurons will be same which will effectively be like having single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its better to initialize bias terms randomly, but it may not have much effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- have non zero derivative at all points, so gradient descent will not stuck and avoid dying neuron\n",
    "- its continuous function which makes gradient descent faster\n",
    "- It outputs -ve value when z(logits) becomes negative. which avoids vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELU and Leaky Relu(and its variants):** can be used in DNN with many layers to avoid vanishing gradient problem. ELU is a good default and give better result leaky Relu can be faster during training and prediction  \n",
    "**ReLU** can be used if we want prediction to be fast  \n",
    "**Hyperbolic Tangent (tanh)** can be used in output layer if output range desired in -1 to 1  \n",
    "**logistic activation** can be used in output layer to output probability   \n",
    "**softmax** can be used in output layer to do multi-class classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with high momentum value gradient descent will take take much speed but it may shoot past global minimum and oscillate around global minimum before settling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- making small weights zero\n",
    "- Using l1 regularization which allows weights to become zero\n",
    "- combining l1 with dual averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yes drouput slow down training because there will be only about half of neurons which will take much time to converge\n",
    "- No, it has not effect on inference(prediction) because it is turned on only during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
