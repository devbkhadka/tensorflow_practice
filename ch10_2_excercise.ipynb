{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "- Perceptron doesn't give class probability instead directly give class\n",
    "- It will converge only when data is linearly separable\n",
    "- It is equivalent to Logistic Regression with constant learning rate of 1 and no regularization\n",
    "\n",
    "If we use logistic function or softmax function as step function of preceptron and train it using gradient descent then it will be equivalent to logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "- It's derivative is always non-zero hence it will converge in gradient descent\n",
    "- It has elegant derivetive f(x)\\*(1-f(x)) which is easy to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name three popular activation functions. Can you draw them?\n",
    "- Sigmoid function\n",
    "- Rellu function\n",
    "- hyperbolic tangent\n",
    "- Step function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the shape of the input matrix X?\n",
    "(m, 10)\n",
    "\n",
    "##### What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "Wh will have 11 dimensions, 1 for bais term and 10 for output of input layer\n",
    "shape of Wh is (10,50)\n",
    "shape of bh is (1, 50)\n",
    "\n",
    "##### What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "Wo will have 51 dimensions, of which 1 is for bais term\n",
    "shape of Wo is (50,3)\n",
    "shape of bo is (1,3)\n",
    "\n",
    "##### What is the shape of the network’s output matrix Y?\n",
    "(m,3)\n",
    "\n",
    "##### Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "\n",
    "(X*Wn+bh)*Wo + bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "\n",
    "- For classifying spam or ham we need only one output neuron? logistic function will be good for it\n",
    "- For MINST there should be 10 output neuron 1 for each digit. Softmax activation function should be used here\n",
    "- Predicting housing price is regerssion problem, there will be one output and no activation function should be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "- Backpropagatio is process of training DNN where as reverse-mode autodiff is method to compute gradient in each step\n",
    "- Backpropagation do a forward pass through the DNN for current training batch and computes output at every neuron. Then it performs a reverse pass to find gradient at once. This process is also called reverse auto diff. Then it minimizes the cost function using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "- Number of hidden layers\n",
    "- Number of neurons in each layer\n",
    "- Activation function of each layer\n",
    "\n",
    "In general relu function will work good in hidden layer and logistic or softmax function will work on output layer\n",
    "\n",
    "If neural network overfits data we can reduce number of layers or number of neurons\n",
    "or we can use early stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
