{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import sqrt\n",
    "\n",
    "\n",
    "\n",
    "def neural_layer(X, n_neurons, layer_name, apply_activation=True):\n",
    "    \n",
    "    n_inputs = int(X.get_shape()[1])\n",
    "    with tf.name_scope(layer_name):\n",
    "        std_dev = 2/sqrt(n_inputs)\n",
    "        init_w = tf.random.truncated_normal((n_inputs, n_neurons),mean=0, stddev=std_dev)\n",
    "        ## each column is weights of a neuron\n",
    "        \n",
    "        w = tf.Variable(init_w, name=\"w\")\n",
    "        b = tf.Variable(tf.zeros((n_neurons)))\n",
    "        \n",
    "        z = tf.matmul(X,w) + b\n",
    "        \n",
    "        if apply_activation:\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "\n",
    "def create_dnn(X, layer_neurons):\n",
    "    last_layer = X\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        for i,n_neurons in enumerate(layer_neurons):\n",
    "            layer_name = \"input_layer\" if i==0 \\\n",
    "                else (\"output_layer\" if i==len(layer_neurons)-1 else \"hidden_layer\"+str(i))\n",
    "            layer = neural_layer(last_layer, n_neurons, layer_name, \n",
    "                                 apply_activation=(i!= len(layer_neurons)-1))\n",
    "            last_layer = layer\n",
    "        \n",
    "    return last_layer\n",
    "    \n",
    "\n",
    "def create_loss(y,dnn):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=dnn)\n",
    "\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_train_op(loss, learning_rate=0.01):\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "    return train_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials import mnist\n",
    "mnist_data = mnist.input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from my_libs.tf_checkpoint import CheckpointSaver\n",
    "from my_libs.tf_graph_saver import ScalerGraphSaver\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "lr=0.01\n",
    "\n",
    "\n",
    "def perform_gradient_descent(train_data, n_epochs, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    n_inputs = 28*28\n",
    "    layer_neurons = [300,200,10]\n",
    "    n_batchs = int(np.ceil(mnist_data.train.num_examples/batch_size))\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, (None, n_inputs) , name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, (None), name=\"y\")\n",
    "\n",
    "    cur_epoch = tf.Variable(0,dtype=tf.int32, name=\"cur_epoch\")\n",
    "    inc_cur_epoch = tf.assign(cur_epoch, cur_epoch+1)\n",
    "\n",
    "    dnn = create_dnn(X,layer_neurons=layer_neurons)\n",
    "    loss = create_loss(y, dnn)\n",
    "    train_op = get_train_op(loss,learning_rate=lr)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_preds = tf.nn.in_top_k(dnn,y,1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_preds,tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        chkpnt_saver = CheckpointSaver(\"tf_dnn_example\")\n",
    "        \n",
    "        if not chkpnt_saver.restore_checkpoint():\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            display(\"check point restored\")\n",
    "        \n",
    "        epoch_start = cur_epoch.eval()\n",
    "        display(\"epoch_start\",epoch_start)\n",
    "        \n",
    "        test_images = mnist_data.test.images\n",
    "        test_labels = mnist_data.test.labels\n",
    "        \n",
    "        with ScalerGraphSaver(\"tf_dnn_example\", loss) as graph_saver:\n",
    "            for epoch in range(epoch_start,n_epochs):\n",
    "                for batch in range(n_batchs):\n",
    "                    X_batch, y_batch = train_data.next_batch(batch_size)\n",
    "\n",
    "                    feed_dict={X:X_batch,y:y_batch}\n",
    "                    sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "                sess.run(inc_cur_epoch)\n",
    "                graph_saver.log_summary(epoch, feed_dict)\n",
    "                if epoch%20==0 or epoch==n_epochs-1:\n",
    "                    print(\"epoch\", epoch)\n",
    "                    chkpnt_saver.save_checkpoint()\n",
    "                    batch_acc = accuracy.eval(feed_dict=feed_dict)\n",
    "                    test_acc = accuracy.eval(feed_dict={X:test_images, y:test_labels})\n",
    "                    display(batch_acc, test_acc)\n",
    "                    \n",
    "    def predict(inp):\n",
    "        with tf.Session() as sess:\n",
    "            chkpnt_saver = CheckpointSaver(\"tf_dnn_example\")\n",
    "            chkpnt_saver.restore_checkpoint()\n",
    "            prob_dist = dnn.eval(feed_dict={X:inp})\n",
    "#             return np.argmax(prob_dist, axis=1)\n",
    "            return prob_dist\n",
    "    \n",
    "    return predict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check point restored'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'epoch_start'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9792"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "predict = perform_gradient_descent(mnist_data.train, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = predict(mnist_data.test.images)\n",
    "# accuracy_score(predictions, mnist_data.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(predictions, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, mnist_data.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
