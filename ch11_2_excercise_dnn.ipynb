{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function. \n",
    "- Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 14:04:36.623637 4760491456 deprecation.py:323] From <ipython-input-1-f648f33866b9>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0819 14:04:36.625555 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0819 14:04:36.627747 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0819 14:04:44.743009 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 14:04:46.018750 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 14:04:49.044633 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials import mnist\n",
    "mnist_data = mnist.input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_indxs_upto_4 = mnist_data.train.labels<5\n",
    "train_images_upto_4 = mnist_data.train.images[train_indxs_upto_4]\n",
    "train_labels_upto_4 = mnist_data.train.labels[train_indxs_upto_4]\n",
    "\n",
    "val_indxs_upto_4 = mnist_data.validation.labels<5\n",
    "val_images_upto_4 = mnist_data.validation.images[val_indxs_upto_4]\n",
    "val_labels_upto_4 = mnist_data.validation.labels[val_indxs_upto_4]\n",
    "\n",
    "test_indxs_upto_4 = mnist_data.test.labels<5\n",
    "test_images_upto_4 = mnist_data.test.images[test_indxs_upto_4]\n",
    "test_labels_upto_4 = mnist_data.test.labels[test_indxs_upto_4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer as he_initializer\n",
    "from tensorflow.nn import sparse_softmax_cross_entropy_with_logits as softmax_xentropy\n",
    "from tensorflow.layers import dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_leaky_relu(alpha):\n",
    "    return lambda z, name=None: tf.maximum(alpha*z,z, name=name)\n",
    "\n",
    "def get_logits(x, layer_sizes):\n",
    "    \n",
    "    activation = get_leaky_relu(alpha=0.3)\n",
    "#     activation = tf.nn.elu\n",
    "    initializer = he_initializer()\n",
    "    \n",
    "    with tf.name_scope(\"DNN\"):\n",
    "        layer_input = x\n",
    "        for i, layer_size in enumerate(layer_sizes[2:]):\n",
    "            layer_name = \"input\" if i==0 else \\\n",
    "                (\"output\" if i==len(layer_sizes)-1 else \"hidden%d\"%i)\n",
    "            \n",
    "            act = None if i==len(layer_sizes)-1 else activation\n",
    "            \n",
    "            layer_input = dense(layer_input, layer_size, name=layer_name, \n",
    "                            kernel_initializer=initializer, activation= act)\n",
    "            \n",
    "    return layer_input\n",
    "\n",
    "\n",
    "def get_softmax_xentropy_loss(logits,y):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = softmax_xentropy(labels=y, logits=logits)\n",
    "        return tf.reduce_mean(xentropy)\n",
    "\n",
    "def get_optimizer(loss, learning_rate=0.01):\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer =  tf.train.AdamOptimizer (learning_rate=learning_rate)\n",
    "        optimizer_op = optimizer.minimize(loss)\n",
    "    return optimizer_op\n",
    "\n",
    "def get_validation_score(logits,y):\n",
    "    with tf.name_scope(\"validation\"):\n",
    "        preds = tf.nn.in_top_k(logits,y,1)\n",
    "        return tf.reduce_mean(tf.cast(preds, dtype=np.float32))\n",
    "    \n",
    "def get_batch(x,y,batch_size):\n",
    "    n_batches = len(y)//batch_size + 1\n",
    "    for i in range(n_batches):\n",
    "        indxes = np.random.choice(len(y), size=batch_size, replace=False)\n",
    "        yield x[indxes], y[indxes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_libs.tf_checkpoint import CheckpointSaver\n",
    "from my_libs.tf_graph_saver import ScalerGraphSaver\n",
    "\n",
    "def train_model(train_ds, validation_ds, restore_checkpoint=True):\n",
    "    layer_sizes = [100,100,100,100,100,5]\n",
    "    learning_rate = 0.01\n",
    "    n_epochs = 300\n",
    "    batch_size = 50\n",
    "    MAX_EPOCHS_WO_IMPROVEMENT = 100\n",
    "\n",
    "    train_x, train_y = train_ds\n",
    "    val_x, val_y = validation_ds\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    var_epoch = tf.Variable(initial_value=0,dtype=np.int16, name=\"epoch\")\n",
    "    inc_epoch = tf.compat.v1.assign_add(var_epoch,1, name=\"inc_epoch\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = tf.placeholder(shape=(None, 28*28), dtype=np.float32,name=\"x\")\n",
    "    y = tf.placeholder(shape=(None), dtype=np.int32,name=\"y\")\n",
    "    logits = get_logits(x, layer_sizes)\n",
    "    \n",
    "    loss_op = get_softmax_xentropy_loss(logits,y)\n",
    "    optimizer = get_optimizer(loss_op, learning_rate)\n",
    "    validation_score = get_validation_score(logits,y)\n",
    "    \n",
    "    var_best_epoch = tf.Variable((0,0), dtype=np.float32, name=\"best_score\")\n",
    "    best_epoch_holder = tf.placeholder(shape=[2], dtype=np.float32, name=\"best_score_holder\")\n",
    "    update_best_epoch = tf.assign(var_best_epoch, best_epoch_holder)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        chk_saver = CheckpointSaver()\n",
    "        \n",
    "        epoch_start=0\n",
    "        best_score = 0\n",
    "        best_score_epoch = 0\n",
    "        if restore_checkpoint and chk_saver.restore_checkpoint(\"excercise1_epoch\"):\n",
    "            epoch_start = var_epoch.eval(sess)\n",
    "            best_score_epoch, best_score = var_best_epoch.eval(sess)\n",
    "            print(\"restored at epoch %d\"%epoch_start)\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "#             init = tf.variables_initializer([var_epoch])\n",
    "            sess.run(init)\n",
    "        \n",
    "        with ScalerGraphSaver(\"excercise1_graph\") as graph_saver:\n",
    "             for epoch in range(epoch_start, n_epochs):\n",
    "                    \n",
    "                if best_score>0 and epoch-best_score_epoch>MAX_EPOCHS_WO_IMPROVEMENT:\n",
    "                    print(\"No improvement in %d epoches restoring best epoch\" \\\n",
    "                          %MAX_EPOCHS_WO_IMPROVEMENT)\n",
    "                    print(\"epoch %d, score %f\"%(best_score_epoch, best_score))\n",
    "                    if chk_saver.restore_checkpoint(\"excercise1_best_epoch\"):\n",
    "                        break \n",
    "                \n",
    "                for batch_x, batch_y in get_batch(train_x,train_y, batch_size):\n",
    "                    feed_dict={x:batch_x,y:batch_y}\n",
    "                    loss,_ = sess.run([loss_op, optimizer], feed_dict = feed_dict)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if (epoch>0 and (epoch%10==0 or epoch==n_epochs-1)):\n",
    "                    chk_saver.save_checkpoint(\"excercise1_epoch\")\n",
    "                    \n",
    "                    graph_saver.log_summary(\"batch_loss\", loss_op, step=epoch, \n",
    "                                            feed_dict=feed_dict)\n",
    "                    graph_saver.log_summary(\"validation_loss\", loss_op, step=epoch, \n",
    "                                            feed_dict={x:val_x,y:val_y})\n",
    "                    \n",
    "                    val_score = validation_score.eval(session=sess, \n",
    "                                                      feed_dict={x:val_x,y:val_y}) \n",
    "                    graph_saver.log_summary(\"validation_accuracy\", validation_score, \n",
    "                                            step=epoch, feed_dict={x:val_x,y:val_y})\n",
    "                    \n",
    "                    if val_score>best_score:\n",
    "                        print(\"best epoch %d, score %f\"%(epoch, val_score))\n",
    "                        best_score = val_score\n",
    "                        sess.run(update_best_epoch, \n",
    "                                 feed_dict={best_epoch_holder:(epoch,val_score)})\n",
    "                        best_score_epoch = epoch\n",
    "                        chk_saver.save_checkpoint(\"excercise1_best_epoch\")\n",
    "                    \n",
    "                      \n",
    "                    \n",
    "                 \n",
    "                sess.run(inc_epoch)   \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(test_x):\n",
    "        with tf.Session() as sess:\n",
    "            chk_saver = CheckpointSaver()\n",
    "            chk_saver.restore_checkpoint(\"excercise1_epoch\")\n",
    "            pred_op = tf.math.argmax(logits, axis=1)\n",
    "            \n",
    "            return sess.run(pred_op, feed_dict={x:test_x})\n",
    "            \n",
    "            \n",
    "         \n",
    "    return predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 14:05:27.398180 4760491456 deprecation.py:323] From <ipython-input-3-de97af167ece>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0819 14:05:28.313132 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0819 14:05:28.850306 4760491456 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/tf_checkpoint.py:8: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0819 14:05:28.851590 4760491456 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/tf_checkpoint.py:9: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W0819 14:05:28.943823 4760491456 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0819 14:05:30.657695 4760491456 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/tf_graph_saver.py:18: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored at epoch 170\n",
      "No improvement in 100 epoches restoring best epoch\n",
      "epoch 70, score 0.987881\n",
      "test accuracy: 0.987741\n"
     ]
    }
   ],
   "source": [
    "predict = train_model((train_images_upto_4,train_labels_upto_4),\n",
    "                      (val_images_upto_4,val_labels_upto_4), restore_checkpoint=True)\n",
    "\n",
    "predictions = predict(test_images_upto_4)\n",
    "accuracy = np.mean(test_labels_upto_4==predictions)\n",
    "print(\"test accuracy: %f\"%accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
