{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new DNN that reuses all the pre-trained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one. \n",
    "- Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? \n",
    "- Try caching the frozen layers, and train the model again: how much faster is it now? \n",
    "- Try again reusing just four hidden layers instead of five. Can you achieve a higher precision? \n",
    "- Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "\n",
    "train_indxes_5to9 = y_train>=5\n",
    "train_x_5to9 = x_train[train_indxes_5to9]\n",
    "train_y_5to9 = y_train[train_indxes_5to9]-5\n",
    "\n",
    "\n",
    "def get_random_indxs(y_train, group_value, count=10):\n",
    "    train_indxs = np.arange(len(y_train), dtype=np.int32)\n",
    "    group_indxs = train_indxs[y_train == group_value]\n",
    "    indxs = np.random.choice(group_indxs,count)\n",
    "    return indxs\n",
    "\n",
    "def get_samples_of_each_group(y_train, count=150):\n",
    "    rand_indxs = np.array([], dtype=np.int32)\n",
    "    for group_val in np.unique(y_train):\n",
    "        rand_indxs=np.r_[rand_indxs,get_random_indxs(y_train,group_val, count)]\n",
    "        \n",
    "    np.random.shuffle(rand_indxs)\n",
    "    return rand_indxs\n",
    "\n",
    "rand_indxs = get_samples_of_each_group(train_y_5to9)\n",
    "train_x_5to9 = train_x_5to9[rand_indxs]\n",
    "train_y_5to9 = train_y_5to9[rand_indxs]\n",
    "\n",
    "\n",
    "train_x_5to9, val_x_5to9, train_y_5to9, val_y_5to9 = \\\n",
    "    train_test_split(train_x_5to9, train_y_5to9, test_size=.3, stratify=train_y_5to9)\n",
    "\n",
    "val_x_5to9, test_x_5to9, val_y_5to9, test_y_5to9 = \\\n",
    "    train_test_split(val_x_5to9,val_y_5to9, test_size=.6, stratify=val_y_5to9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that DNN_Classifier is not broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 20:16:37.442067 4489565632 deprecation.py:323] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0827 20:16:38.038173 4489565632 deprecation.py:323] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:32: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0827 20:16:38.363252 4489565632 deprecation.py:506] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0827 20:16:39.011906 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:112: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.677778, loss 0.230127\n",
      "epoch 20, score 0.877778, loss 0.004077\n",
      "epoch 40, score 0.922222, loss 0.001653\n",
      "epoch 60, score 0.888889, loss 0.000041\n",
      "epoch 80, score 0.922222, loss 0.000076\n",
      "epoch 100, score 0.933333, loss 0.000510\n",
      "epoch 120, score 0.922222, loss 0.000132\n",
      "epoch 140, score 0.933333, loss 0.004663\n",
      "epoch 160, score 0.933333, loss 0.000084\n",
      "epoch 180, score 0.911111, loss 0.000024\n",
      "epoch 200, score 0.911111, loss 0.000003\n",
      "epoch 220, score 0.866667, loss 0.181322\n",
      "epoch 240, score 0.955556, loss 0.000023\n",
      "No progress for 50 epoches.\n",
      "Reverting back to epoch 202                     with 0.000001 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier(activation=<function elu at 0x1a21845268>,\n",
       "        batch_norm_momentum=0.95, batch_size=50, dropout_rate=None,\n",
       "        learning_rate=0.01, n_hidden_layers=3, n_neurons=100, n_outputs=5,\n",
       "        optimizer=<class 'tensorflow.python.training.adam.AdamOptimizer'>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from imp import reload\n",
    "# import my_libs\n",
    "# reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "classifier = DNN_Classifier(n_hidden_layers=3, n_neurons=100,n_outputs=5,\n",
    "                            batch_norm_momentum=.95)\n",
    "\n",
    "classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN_Classifier_Transfer\n",
    "which will transfer learning of DNN_Classifier from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "\n",
    "import tensorflow as tf\n",
    "from my_libs.dnn import DNN_Classifier, get_optimizer_op, get_validation_score, \\\n",
    "    get_softmax_xentropy_loss\n",
    "from tensorflow.train import AdamOptimizer\n",
    "\n",
    "class DNN_Classifier_Transfer(DNN_Classifier):\n",
    "    def __init__(self, checkpoint_name, use_hidden_layers=0):\n",
    "        DNN_Classifier.__init__(self, batch_size=20)\n",
    "        self._checkpoint_name = checkpoint_name\n",
    "        self._batch_norm_update_ops = None\n",
    "        self.use_hidden_layers = use_hidden_layers\n",
    "        \n",
    "    \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier._restore_graph(self,self._checkpoint_name)\n",
    "        \n",
    "        self.restore_n_hidden_layers()\n",
    "        \n",
    "        get_layer_name = lambda i: \"hidden%d\"%(self.n_hidden_layers-i)\n",
    "        variable_scopes = [get_layer_name(i) for i in range(self.use_hidden_layers)]\n",
    "        \n",
    "        trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"output\")\n",
    "        \n",
    "        for scope in variable_scopes:\n",
    "            trainable_variables.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=scope))\n",
    "        \n",
    "        optimizer = AdamOptimizer(0.01, name=\"adam2\")\n",
    "        self._optimizer_op = optimizer.minimize(self._loss, var_list=trainable_variables)\n",
    "        \n",
    "        DNN_Classifier._restore_session(self, self._checkpoint_name)\n",
    "        return self._graph\n",
    "    \n",
    "    def restore_n_hidden_layers(self):\n",
    "        i=1\n",
    "        tensors = []\n",
    "        while True:\n",
    "            try:       \n",
    "                tensor = self._graph.get_tensor_by_name(\"DNN/hiden%d_out:0\"%i)\n",
    "                tensors.append(tensor)\n",
    "                i+=1\n",
    "            except:\n",
    "                break\n",
    "        self.n_hidden_layers = i-1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.300000, loss 14.546036\n",
      "epoch 20, score 0.322222, loss 1.155059\n",
      "epoch 40, score 0.377778, loss 1.077784\n",
      "No progress for 20 epoches.\n",
      "Reverting back to epoch 27                     with 0.765974 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier_Transfer(checkpoint_name=None, use_hidden_layers=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(5)\n",
    "# tf.random.set_random_seed(5)\n",
    "\n",
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\", use_hidden_layers=1)\n",
    "transfer_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crate DNN_Classifier_Frozen\n",
    "- Same as DNN_Classifier_Transfer but it will pre calculate output of previous layer for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "class DNN_Classifier_Frozen(DNN_Classifier_Transfer):\n",
    "        \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier_Transfer._initialize_session_and_graph(self)\n",
    "        forzen_output_name = \"DNN/hiden%d_out:0\"%(self.n_hidden_layers-self.use_hidden_layers)\n",
    "        print(forzen_output_name)\n",
    "        self._frozen_out = self._graph.get_tensor_by_name(forzen_output_name)\n",
    "        ## replace input placeholder _x with output of hidden layer _frozen_out\n",
    "        self._old_x = self._x\n",
    "        self._x = self._frozen_out\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self._initialize_session_and_graph()\n",
    "        with self._session.as_default() as sess:\n",
    "            self._x_frozen_out = self._frozen_out.eval(session=sess,feed_dict={self._old_x:x})\n",
    "            self._val_x_frozen_out = self._frozen_out.eval(feed_dict={self._old_x:val_x})\n",
    "            \n",
    "        return DNN_Classifier.fit(self, self._x_frozen_out, y, self._val_x_frozen_out, val_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN/hiden4_out:0\n",
      "DNN/hiden4_out:0\n",
      "epoch 0, score 0.311111, loss 9.336866\n",
      "epoch 20, score 0.344444, loss 0.940411\n",
      "epoch 40, score 0.366667, loss 0.693604\n",
      "epoch 60, score 0.388889, loss 0.702118\n",
      "epoch 80, score 0.388889, loss 0.819313\n",
      "epoch 100, score 0.400000, loss 0.557305\n",
      "epoch 120, score 0.355556, loss 0.858217\n",
      "epoch 140, score 0.366667, loss 0.367672\n",
      "epoch 160, score 0.311111, loss 0.416599\n",
      "epoch 180, score 0.288889, loss 0.435702\n",
      "epoch 200, score 0.222222, loss 0.894056\n",
      "No progress for 50 epoches.\n",
      "Reverting back to epoch 165                     with loss: 0.155090, val accuracy 0.322222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier_Frozen(checkpoint_name=None, use_hidden_layers=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "frozen_classifier = DNN_Classifier_Frozen(\"Mnist-0to4-best_batch_norm\", use_hidden_layers=1)\n",
    "frozen_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if running variable initializer resets previously set value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "xx = tf.Variable(0, dtype=np.int16, name=\"xx\")\n",
    "inc = tf.assign(xx, xx+1)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run(inc)\n",
    "    sess.run(init)\n",
    "    print(xx.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 09:49:44.197348 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:223: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0827 09:49:44.199877 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:224: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "W0827 09:49:45.264413 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:225: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0827 09:49:45.501465 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:240: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0827 09:49:45.504031 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:242: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0827 09:49:45.923485 4536214976 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\")\n",
    "graph=transfer_classifier._initialize_session_and_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNN/hiden1_out',\n",
       " 'DNN/hiden2_out',\n",
       " 'DNN/hiden3_out',\n",
       " 'DNN/hiden4_out',\n",
       " 'DNN/hiden5_out']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[op.name for op in graph.get_operations() if ptrn.search(op.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(4, 14), match='hiden5_out'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ptrn=re.compile(r\"hiden\\d+_out$\")\n",
    "print(ptrn.search(\"DNN/hiden5_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_hidden_layers(graph):\n",
    "    i=1\n",
    "    tensors = []\n",
    "    while True:\n",
    "        try:       \n",
    "            tensor = graph.get_tensor_by_name(\"DNN/hiden%d_out:0\"%i)\n",
    "            tensors.append(tensor)\n",
    "            i+=1\n",
    "        except:\n",
    "            break\n",
    "    return i-1\n",
    "\n",
    "get_n_hidden_layers(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden5/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/bias:0' shape=(150,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"hidden5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
