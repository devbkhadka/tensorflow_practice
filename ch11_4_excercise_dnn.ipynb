{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new DNN that reuses all the pre-trained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one. \n",
    "- Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? \n",
    "- Try caching the frozen layers, and train the model again: how much faster is it now? \n",
    "- Try again reusing just four hidden layers instead of five. Can you achieve a higher precision? \n",
    "- Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "\n",
    "train_indxes_5to9 = y_train>=5\n",
    "train_x_5to9 = x_train[train_indxes_5to9]\n",
    "train_y_5to9 = y_train[train_indxes_5to9]-5\n",
    "\n",
    "\n",
    "def get_random_indxs(y_train, group_value, count=10):\n",
    "    train_indxs = np.arange(len(y_train), dtype=np.int32)\n",
    "    group_indxs = train_indxs[y_train == group_value]\n",
    "    indxs = np.random.choice(group_indxs,count)\n",
    "    return indxs\n",
    "\n",
    "def get_samples_of_each_group(y_train, count=300):\n",
    "    rand_indxs = np.array([], dtype=np.int32)\n",
    "    for group_val in np.unique(y_train):\n",
    "        rand_indxs=np.r_[rand_indxs,get_random_indxs(y_train,group_val, count)]\n",
    "        \n",
    "    np.random.shuffle(rand_indxs)\n",
    "    return rand_indxs\n",
    "\n",
    "rand_indxs = get_samples_of_each_group(train_y_5to9)\n",
    "\n",
    "train_x_5to9, val_x_5to9, train_y_5to9, val_y_5to9 = \\\n",
    "    train_test_split(train_x_5to9, train_y_5to9, test_size=.3)\n",
    "\n",
    "val_x_5to9, test_x_5to9, val_y_5to9, test_y_5to9 = \\\n",
    "    train_test_split(val_x_5to9,val_y_5to9, test_size=.6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that DNN_Classifier is not broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "classifier = DNN_Classifier(n_hidden_layers=3, n_neurons=100,n_outputs=5,\n",
    "                            batch_norm_momentum=.95)\n",
    "\n",
    "classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN_Classifier_Transfer\n",
    "which will transfer learning of DNN_Classifier from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "\n",
    "import tensorflow as tf\n",
    "from my_libs.dnn import DNN_Classifier, get_optimizer_op, get_validation_score, \\\n",
    "    get_softmax_xentropy_loss\n",
    "from tensorflow.train import AdamOptimizer\n",
    "\n",
    "class DNN_Classifier_Transfer(DNN_Classifier):\n",
    "    def __init__(self, checkpoint_name, reuse_layers=1):\n",
    "        DNN_Classifier.__init__(self)\n",
    "        self._checkpoint_name = checkpoint_name\n",
    "        self._batch_norm_update_ops = None\n",
    "        \n",
    "    \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier._restore_graph(self,self._checkpoint_name)\n",
    "        \n",
    "        trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"output\")\n",
    "        \n",
    "        optimizer = AdamOptimizer(0.01, name=\"adam2\")\n",
    "        self._optimizer_op = optimizer.minimize(self._loss, var_list=trainable_variables)\n",
    "        \n",
    "        DNN_Classifier._restore_session(self, self._checkpoint_name)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "tf.random.set_random_seed(40)\n",
    "\n",
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\")\n",
    "transfer_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crate DNN_Classifier_Frozen\n",
    "- Same as DNN_Classifier_Transfer but it will pre calculate output of previous layer for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "class DNN_Classifier_Frozen(DNN_Classifier_Transfer):\n",
    "        \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier_Transfer._initialize_session_and_graph(self)\n",
    "        self._frozen_out = self._graph.get_tensor_by_name(\"DNN/hiden5_out:0\")\n",
    "        ## replace input placeholder _x with output of hidden layer _frozen_out\n",
    "        self._old_x = self._x\n",
    "        self._x = self._frozen_out\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self._initialize_session_and_graph()\n",
    "        with self._session.as_default() as sess:\n",
    "            self._x_frozen_out = self._frozen_out.eval(session=sess,feed_dict={self._old_x:x})\n",
    "            self._val_x_frozen_out = self._frozen_out.eval(feed_dict={self._old_x:val_x})\n",
    "            \n",
    "        return DNN_Classifier.fit(self, self._x_frozen_out, y, self._val_x_frozen_out, val_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.501984, loss 1.376232\n",
      "epoch 50, score 0.602891, loss 0.947225\n",
      "epoch 100, score 0.603175, loss 1.030845\n",
      "epoch 150, score 0.606576, loss 0.932684\n",
      "epoch 200, score 0.602324, loss 0.827789\n",
      "epoch 250, score 0.600340, loss 0.854857\n",
      "No progress for 100 epoches.\n",
      "Reverting back to epoch 189                     with 0.615646 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier_Frozen(checkpoint_name=None, reuse_layers=None)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "tf.random.set_random_seed(40)\n",
    "\n",
    "frozen_classifier = DNN_Classifier_Frozen(\"Mnist-0to4-best_batch_norm\")\n",
    "frozen_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if running variable initializer resets previously set value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "xx = tf.Variable(0, dtype=np.int16, name=\"xx\")\n",
    "inc = tf.assign(xx, xx+1)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run(inc)\n",
    "    sess.run(init)\n",
    "    print(xx.eval())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
