{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new DNN that reuses all the pre-trained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one. \n",
    "- Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? \n",
    "- Try caching the frozen layers, and train the model again: how much faster is it now? \n",
    "- Try again reusing just four hidden layers instead of five. Can you achieve a higher precision? \n",
    "- Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(np.float32).reshape(-1, 28*28)/255.0\n",
    "\n",
    "train_indxes_5to9 = y_train>=5\n",
    "train_x_5to9 = x_train[train_indxes_5to9]\n",
    "train_y_5to9 = y_train[train_indxes_5to9]-5\n",
    "\n",
    "\n",
    "def get_random_indxs(y_train, group_value, count=10):\n",
    "    train_indxs = np.arange(len(y_train), dtype=np.int32)\n",
    "    group_indxs = train_indxs[y_train == group_value]\n",
    "    indxs = np.random.choice(group_indxs,count)\n",
    "    return indxs\n",
    "\n",
    "def get_samples_of_each_group(y_train, count=400):\n",
    "    rand_indxs = np.array([], dtype=np.int32)\n",
    "    for group_val in np.unique(y_train):\n",
    "        rand_indxs=np.r_[rand_indxs,get_random_indxs(y_train,group_val, count)]\n",
    "        \n",
    "#     np.random.shuffle(rand_indxs)\n",
    "    return rand_indxs\n",
    "\n",
    "rand_indxs = get_samples_of_each_group(train_y_5to9, count=500)\n",
    "train_x_5to9 = train_x_5to9[rand_indxs]\n",
    "train_y_5to9 = train_y_5to9[rand_indxs]\n",
    "\n",
    "\n",
    "train_x_5to9, val_x_5to9, train_y_5to9, val_y_5to9 = \\\n",
    "    train_test_split(train_x_5to9, train_y_5to9, test_size=.66, stratify=train_y_5to9)\n",
    "\n",
    "val_x_5to9, test_x_5to9, val_y_5to9, test_y_5to9 = \\\n",
    "    train_test_split(val_x_5to9,val_y_5to9, test_size=.6, stratify=val_y_5to9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=uint8), array([132, 132, 132, 132, 132]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(val_y_5to9, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN_Classifier_Transfer\n",
    "which will transfer learning of DNN_Classifier from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs.dnn\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import get_leaky_relu\n",
    "import tensorflow as tf\n",
    "from my_libs.dnn import DNN_Classifier, get_optimizer_op, get_validation_score, \\\n",
    "    get_softmax_xentropy_loss, get_batch\n",
    "from tensorflow.train import AdamOptimizer, MomentumOptimizer\n",
    "\n",
    "class DNN_Classifier_Transfer(DNN_Classifier):\n",
    "    def __init__(self, checkpoint_name, use_hidden_layers=0):\n",
    "        DNN_Classifier.__init__(self)\n",
    "        self._checkpoint_name = checkpoint_name\n",
    "        self._batch_norm_update_ops = None\n",
    "        self.use_hidden_layers = use_hidden_layers\n",
    "        \n",
    "    \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier._restore_graph(self,self._checkpoint_name)\n",
    "        \n",
    "        self.restore_n_hidden_layers()\n",
    "        \n",
    "         \n",
    "        trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"output\")\n",
    "        self._batch_norm_update_ops = self._graph.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        for i in range(self.n_hidden_layers, self.use_hidden_layers, -1):\n",
    "            layer_scope = \"hidden%d\"%(i)\n",
    "            layer_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=layer_scope)\n",
    "            trainable_variables = trainable_variables+layer_variables\n",
    "            \n",
    "        for i in range(self.n_hidden_layers, self.use_hidden_layers, -1):\n",
    "            if len(self._batch_norm_update_ops)>0:\n",
    "                batch_norm_scope = \"batch_normalization_%d\"%(i-1)\n",
    "                batch_norm_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                    scope= batch_norm_scope)\n",
    "                trainable_variables = trainable_variables+batch_norm_variables\n",
    "            \n",
    "            \n",
    "        print(trainable_variables)\n",
    "        \n",
    "        optimizer = AdamOptimizer(0.01, name=\"adam2\")\n",
    "        self._optimizer_op = optimizer.minimize(self._loss, var_list=trainable_variables)\n",
    "        \n",
    "        DNN_Classifier._restore_session(self, self._checkpoint_name)\n",
    "        return self._graph\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self._initialize_session_and_graph()\n",
    "        \n",
    "        with self._session.as_default() as sess:\n",
    "            for epoch in range(100):\n",
    "                for batch_x, batch_y in get_batch(x,y, 10):\n",
    "                    ops = [self._loss, self._optimizer_op]\n",
    "                    if len(self._batch_norm_update_ops)>0:\n",
    "                        opts = ops+self._batch_norm_update_ops\n",
    "                        \n",
    "                    res = sess.run(ops, \n",
    "                            feed_dict={\n",
    "                                self._x: batch_x,\n",
    "                                self._y: batch_y,\n",
    "                                self._is_training: True\n",
    "                            })\n",
    "                \n",
    "                tloss = res[0]\n",
    "                score, vloss = sess.run([self._validation_score, self._loss], feed_dict={\n",
    "                                self._x: val_x,\n",
    "                                self._y: val_y,\n",
    "                                self._is_training: False\n",
    "                            })\n",
    "                print(\"epoch: %d, train loss: %f, score: %f, loss: %f\"%(epoch, tloss, score, vloss))\n",
    "    \n",
    "    def restore_n_hidden_layers(self):\n",
    "        i=1\n",
    "        tensors = []\n",
    "        while True:\n",
    "            try:       \n",
    "                tensor = self._graph.get_tensor_by_name(\"DNN/hiden%d_out:0\"%i)\n",
    "                tensors.append(tensor)\n",
    "                i+=1\n",
    "            except:\n",
    "                break\n",
    "        self.n_hidden_layers = i-1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'output/kernel:0' shape=(100, 5) dtype=float32_ref>, <tf.Variable 'output/bias:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'hidden5/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden5/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden4/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden4/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden3/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden3/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden2/kernel:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'hidden2/bias:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'hidden1/kernel:0' shape=(784, 100) dtype=float32_ref>, <tf.Variable 'hidden1/bias:0' shape=(100,) dtype=float32_ref>]\n",
      "epoch: 0, train loss: 0.894499, score: 0.598485, loss: 1.125900\n",
      "epoch: 1, train loss: 0.372319, score: 0.681818, loss: 2.012900\n",
      "epoch: 2, train loss: 0.576928, score: 0.721212, loss: 0.885489\n",
      "epoch: 3, train loss: 0.505067, score: 0.672727, loss: 0.875859\n",
      "epoch: 4, train loss: 0.443660, score: 0.801515, loss: 0.687746\n",
      "epoch: 5, train loss: 27.334509, score: 0.653030, loss: 2.512123\n",
      "epoch: 6, train loss: 1.070824, score: 0.406061, loss: 1.463516\n",
      "epoch: 7, train loss: 0.937935, score: 0.504545, loss: 1.318889\n",
      "epoch: 8, train loss: 1.351661, score: 0.357576, loss: 2.012770\n",
      "epoch: 9, train loss: 1.204631, score: 0.562121, loss: 1.097543\n",
      "epoch: 10, train loss: 23.381197, score: 0.627273, loss: 5.179578\n",
      "epoch: 11, train loss: 1.247426, score: 0.586364, loss: 1.221333\n",
      "epoch: 12, train loss: 0.414717, score: 0.654545, loss: 0.879580\n",
      "epoch: 13, train loss: 0.891030, score: 0.787879, loss: 0.587585\n",
      "epoch: 14, train loss: 0.652849, score: 0.793939, loss: 0.614529\n",
      "epoch: 15, train loss: 0.094612, score: 0.830303, loss: 0.505097\n",
      "epoch: 16, train loss: 0.341367, score: 0.756061, loss: 0.763052\n",
      "epoch: 17, train loss: 0.708693, score: 0.580303, loss: 0.968364\n",
      "epoch: 18, train loss: 0.035483, score: 0.774242, loss: 0.963237\n",
      "epoch: 19, train loss: 0.043948, score: 0.862121, loss: 0.415053\n",
      "epoch: 20, train loss: 0.036959, score: 0.880303, loss: 0.463145\n",
      "epoch: 21, train loss: 0.008102, score: 0.871212, loss: 2.246558\n",
      "epoch: 22, train loss: 0.005289, score: 0.900000, loss: 2.173031\n",
      "epoch: 23, train loss: 1.362350, score: 0.486364, loss: 1.743873\n",
      "epoch: 24, train loss: 3.030371, score: 0.413636, loss: 1.928577\n",
      "epoch: 25, train loss: 2.125856, score: 0.513636, loss: 2.747983\n",
      "epoch: 26, train loss: 1.168664, score: 0.433333, loss: 2.235200\n",
      "epoch: 27, train loss: 0.575940, score: 0.630303, loss: 1.135424\n",
      "epoch: 28, train loss: 0.789566, score: 0.660606, loss: 0.867462\n",
      "epoch: 29, train loss: 0.655656, score: 0.816667, loss: 0.574552\n",
      "epoch: 30, train loss: 0.404042, score: 0.716667, loss: 0.651585\n",
      "epoch: 31, train loss: 0.251787, score: 0.854545, loss: 0.463118\n",
      "epoch: 32, train loss: 0.101093, score: 0.896970, loss: 0.365574\n",
      "epoch: 33, train loss: 0.044564, score: 0.909091, loss: 0.337604\n",
      "epoch: 34, train loss: 0.156290, score: 0.815152, loss: 1.054336\n",
      "epoch: 35, train loss: 0.104604, score: 0.857576, loss: 0.595523\n",
      "epoch: 36, train loss: 0.520607, score: 0.846970, loss: 0.573166\n",
      "epoch: 37, train loss: 0.010184, score: 0.904545, loss: 0.553841\n",
      "epoch: 38, train loss: 0.519673, score: 0.754545, loss: 2.576938\n",
      "epoch: 39, train loss: 0.072998, score: 0.871212, loss: 0.676331\n",
      "epoch: 40, train loss: 0.036490, score: 0.898485, loss: 0.417583\n",
      "epoch: 41, train loss: 0.041037, score: 0.903030, loss: 0.472109\n",
      "epoch: 42, train loss: 0.469236, score: 0.706061, loss: 1.362681\n",
      "epoch: 43, train loss: 0.033795, score: 0.877273, loss: 1.837661\n",
      "epoch: 44, train loss: 0.198006, score: 0.869697, loss: 3.861279\n",
      "epoch: 45, train loss: 0.037733, score: 0.900000, loss: 4.015148\n",
      "epoch: 46, train loss: 0.112152, score: 0.921212, loss: 3.487530\n",
      "epoch: 47, train loss: 0.007505, score: 0.900000, loss: 7.925001\n",
      "epoch: 48, train loss: 0.633430, score: 0.745455, loss: 0.728159\n",
      "epoch: 49, train loss: 0.360592, score: 0.866667, loss: 1.208388\n",
      "epoch: 50, train loss: 0.040038, score: 0.845455, loss: 0.657524\n",
      "epoch: 51, train loss: 0.275486, score: 0.836364, loss: 2.732597\n",
      "epoch: 52, train loss: 0.036031, score: 0.859091, loss: 2.428573\n",
      "epoch: 53, train loss: 0.600487, score: 0.765152, loss: 1.012787\n",
      "epoch: 54, train loss: 0.638570, score: 0.733333, loss: 2.280169\n",
      "epoch: 55, train loss: 0.774917, score: 0.643939, loss: 4.034893\n",
      "epoch: 56, train loss: 0.925797, score: 0.531818, loss: 1.568240\n",
      "epoch: 57, train loss: 0.528544, score: 0.762121, loss: 0.643177\n",
      "epoch: 58, train loss: 0.127759, score: 0.818182, loss: 0.838695\n",
      "epoch: 59, train loss: 0.798757, score: 0.577273, loss: 1.353639\n",
      "epoch: 60, train loss: 0.075932, score: 0.839394, loss: 0.502690\n",
      "epoch: 61, train loss: 1.344114, score: 0.443939, loss: 2.157867\n",
      "epoch: 62, train loss: 1.866693, score: 0.472727, loss: 1.092619\n",
      "epoch: 63, train loss: 1.227280, score: 0.462121, loss: 9.796052\n",
      "epoch: 64, train loss: 0.770415, score: 0.653030, loss: 3.560061\n",
      "epoch: 65, train loss: 0.889315, score: 0.654545, loss: 0.998590\n",
      "epoch: 66, train loss: 5.197914, score: 0.398485, loss: 4.455209\n",
      "epoch: 67, train loss: 0.238761, score: 0.753030, loss: 0.623164\n",
      "epoch: 68, train loss: 0.269352, score: 0.831818, loss: 0.514005\n",
      "epoch: 69, train loss: 0.348331, score: 0.842424, loss: 0.621376\n",
      "epoch: 70, train loss: 0.803142, score: 0.596970, loss: 1.244385\n",
      "epoch: 71, train loss: 4.297172, score: 0.366667, loss: 5.546064\n",
      "epoch: 72, train loss: 0.502225, score: 0.728788, loss: 0.819855\n",
      "epoch: 73, train loss: 0.815558, score: 0.831818, loss: 0.608184\n",
      "epoch: 74, train loss: 0.648700, score: 0.809091, loss: 0.846666\n",
      "epoch: 75, train loss: 0.581497, score: 0.783333, loss: 0.909650\n",
      "epoch: 76, train loss: 0.515527, score: 0.846970, loss: 0.663403\n",
      "epoch: 77, train loss: 0.040912, score: 0.890909, loss: 0.440885\n",
      "epoch: 78, train loss: 0.298411, score: 0.884848, loss: 0.519107\n",
      "epoch: 79, train loss: 0.063521, score: 0.878788, loss: 0.503381\n",
      "epoch: 80, train loss: 0.091868, score: 0.887879, loss: 0.458940\n",
      "epoch: 81, train loss: 0.006853, score: 0.892424, loss: 0.518767\n",
      "epoch: 82, train loss: 0.055062, score: 0.875758, loss: 0.581851\n",
      "epoch: 83, train loss: 0.103624, score: 0.877273, loss: 0.593007\n",
      "epoch: 84, train loss: 0.056107, score: 0.874242, loss: 0.573344\n",
      "epoch: 85, train loss: 0.077656, score: 0.889394, loss: 0.587706\n",
      "epoch: 86, train loss: 0.738577, score: 0.904545, loss: 0.526669\n",
      "epoch: 87, train loss: 0.001927, score: 0.850000, loss: 0.861307\n",
      "epoch: 88, train loss: 0.000213, score: 0.893939, loss: 2.045378\n",
      "epoch: 89, train loss: 1.030256, score: 0.604545, loss: 1.857150\n",
      "epoch: 90, train loss: 0.436235, score: 0.884848, loss: 0.478102\n",
      "epoch: 91, train loss: 0.016000, score: 0.898485, loss: 0.565017\n",
      "epoch: 92, train loss: 0.001400, score: 0.880303, loss: 0.676713\n",
      "epoch: 93, train loss: 0.449919, score: 0.856061, loss: 1.314608\n",
      "epoch: 94, train loss: 0.030474, score: 0.845455, loss: 0.664070\n",
      "epoch: 95, train loss: 0.083030, score: 0.900000, loss: 3.633768\n",
      "epoch: 96, train loss: 0.000091, score: 0.893939, loss: 3.801571\n",
      "epoch: 97, train loss: 0.016606, score: 0.909091, loss: 3.948469\n",
      "epoch: 98, train loss: 0.014820, score: 0.901515, loss: 4.644538\n",
      "epoch: 99, train loss: 0.005217, score: 0.918182, loss: 4.161435\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(5)\n",
    "# tf.random.set_random_seed(5)\n",
    "\n",
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best1\", \n",
    "                                              use_hidden_layers=0)\n",
    "transfer_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'output/kernel:0' shape=(120, 5) dtype=float32_ref>, <tf.Variable 'output/bias:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'hidden5/kernel:0' shape=(120, 120) dtype=float32_ref>, <tf.Variable 'hidden5/bias:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/gamma:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/beta:0' shape=(120,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\", \n",
    "                                              use_hidden_layers=4)\n",
    "graph = transfer_classifier._initialize_session_and_graph()\n",
    "\n",
    "var1= graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"output\")[1]\n",
    "# tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"batch_normalization\")\n",
    "# graph.get_collection(scope=\"DNN/batch_normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RefVariable.value of <tf.Variable 'output/bias:0' shape=(5,) dtype=float32_ref>>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN_Classifier_Frozen\n",
    "- Same as DNN_Classifier_Transfer but it will pre calculate output of previous layer for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "class DNN_Classifier_Frozen(DNN_Classifier_Transfer):\n",
    "        \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier_Transfer._initialize_session_and_graph(self)\n",
    "        frozen_output_name = \"DNN/hiden%d_out:0\"%(self.use_hidden_layers)\n",
    "        self._frozen_out = self._graph.get_tensor_by_name(frozen_output_name)\n",
    "        ## replace input placeholder _x with output of hidden layer _frozen_out\n",
    "        self._old_x = self._x\n",
    "        self._x = self._frozen_out\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self._initialize_session_and_graph()\n",
    "        with self._session.as_default() as sess:\n",
    "            self._x_frozen_out = self._frozen_out.eval(session=sess,feed_dict={self._old_x:x})\n",
    "            self._val_x_frozen_out = self._frozen_out.eval(feed_dict={self._old_x:val_x})\n",
    "            \n",
    "        return DNN_Classifier_Transfer.fit(self, self._x_frozen_out, y, self._val_x_frozen_out, val_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'output/kernel:0' shape=(120, 5) dtype=float32_ref>, <tf.Variable 'output/bias:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'hidden5/kernel:0' shape=(120, 120) dtype=float32_ref>, <tf.Variable 'hidden5/bias:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'hidden4/kernel:0' shape=(120, 120) dtype=float32_ref>, <tf.Variable 'hidden4/bias:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/gamma:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/beta:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_3/gamma:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_3/beta:0' shape=(120,) dtype=float32_ref>]\n",
      "[<tf.Variable 'output/kernel:0' shape=(120, 5) dtype=float32_ref>, <tf.Variable 'output/bias:0' shape=(5,) dtype=float32_ref>, <tf.Variable 'hidden5/kernel:0' shape=(120, 120) dtype=float32_ref>, <tf.Variable 'hidden5/bias:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'hidden4/kernel:0' shape=(120, 120) dtype=float32_ref>, <tf.Variable 'hidden4/bias:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/gamma:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_4/beta:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_3/gamma:0' shape=(120,) dtype=float32_ref>, <tf.Variable 'batch_normalization_3/beta:0' shape=(120,) dtype=float32_ref>]\n",
      "epoch: 0, train loss: 0.833834, score: 0.434848, loss: 2.562739\n",
      "epoch: 1, train loss: 2.289135, score: 0.387879, loss: 1.678492\n",
      "epoch: 2, train loss: 1.591146, score: 0.404545, loss: 1.584324\n",
      "epoch: 3, train loss: 0.921889, score: 0.475758, loss: 1.366124\n",
      "epoch: 4, train loss: 1.324694, score: 0.496970, loss: 1.223927\n",
      "epoch: 5, train loss: 1.711398, score: 0.471212, loss: 1.323374\n",
      "epoch: 6, train loss: 1.279720, score: 0.412121, loss: 1.253213\n",
      "epoch: 7, train loss: 1.421928, score: 0.521212, loss: 1.167116\n",
      "epoch: 8, train loss: 0.587563, score: 0.446970, loss: 1.358614\n",
      "epoch: 9, train loss: 1.002367, score: 0.424242, loss: 1.355989\n",
      "epoch: 10, train loss: 0.601104, score: 0.498485, loss: 1.289175\n",
      "epoch: 11, train loss: 1.070772, score: 0.521212, loss: 1.126713\n",
      "epoch: 12, train loss: 1.313285, score: 0.484848, loss: 1.312500\n",
      "epoch: 13, train loss: 1.510110, score: 0.475758, loss: 1.155554\n",
      "epoch: 14, train loss: 0.851744, score: 0.451515, loss: 1.237018\n",
      "epoch: 15, train loss: 0.782032, score: 0.501515, loss: 1.357610\n",
      "epoch: 16, train loss: 0.358831, score: 0.504545, loss: 1.183182\n",
      "epoch: 17, train loss: 0.441064, score: 0.518182, loss: 1.387745\n",
      "epoch: 18, train loss: 1.179651, score: 0.463636, loss: 1.294542\n",
      "epoch: 19, train loss: 0.743922, score: 0.484848, loss: 1.291232\n",
      "epoch: 20, train loss: 0.571330, score: 0.539394, loss: 1.125167\n",
      "epoch: 21, train loss: 0.804589, score: 0.460606, loss: 1.340757\n",
      "epoch: 22, train loss: 0.420600, score: 0.442424, loss: 1.220594\n",
      "epoch: 23, train loss: 0.412714, score: 0.442424, loss: 1.261399\n",
      "epoch: 24, train loss: 0.882180, score: 0.477273, loss: 1.175532\n",
      "epoch: 25, train loss: 0.659186, score: 0.525758, loss: 1.154662\n",
      "epoch: 26, train loss: 0.884479, score: 0.445455, loss: 1.284944\n",
      "epoch: 27, train loss: 0.534094, score: 0.407576, loss: 1.459842\n",
      "epoch: 28, train loss: 0.774461, score: 0.471212, loss: 1.292543\n",
      "epoch: 29, train loss: 0.642487, score: 0.445455, loss: 1.264176\n",
      "epoch: 30, train loss: 0.692108, score: 0.478788, loss: 1.151187\n",
      "epoch: 31, train loss: 1.137148, score: 0.453030, loss: 1.293257\n",
      "epoch: 32, train loss: 0.301195, score: 0.433333, loss: 1.285832\n",
      "epoch: 33, train loss: 0.419539, score: 0.474242, loss: 1.190069\n",
      "epoch: 34, train loss: 0.583095, score: 0.490909, loss: 1.216854\n",
      "epoch: 35, train loss: 0.229794, score: 0.501515, loss: 1.141887\n",
      "epoch: 36, train loss: 0.529499, score: 0.506061, loss: 1.208665\n",
      "epoch: 37, train loss: 1.053629, score: 0.478788, loss: 1.134878\n",
      "epoch: 38, train loss: 0.434614, score: 0.466667, loss: 1.162073\n",
      "epoch: 39, train loss: 0.764843, score: 0.431818, loss: 1.227638\n",
      "epoch: 40, train loss: 0.422582, score: 0.477273, loss: 1.186303\n",
      "epoch: 41, train loss: 1.013743, score: 0.480303, loss: 1.212683\n",
      "epoch: 42, train loss: 0.646356, score: 0.415152, loss: 1.283548\n",
      "epoch: 43, train loss: 0.801515, score: 0.413636, loss: 1.277368\n",
      "epoch: 44, train loss: 0.635979, score: 0.389394, loss: 1.277770\n",
      "epoch: 45, train loss: 0.310118, score: 0.427273, loss: 1.259551\n",
      "epoch: 46, train loss: 0.289679, score: 0.490909, loss: 1.170747\n",
      "epoch: 47, train loss: 0.658729, score: 0.492424, loss: 1.185341\n",
      "epoch: 48, train loss: 0.498310, score: 0.568182, loss: 1.092057\n",
      "epoch: 49, train loss: 0.393724, score: 0.454545, loss: 1.168879\n",
      "epoch: 50, train loss: 0.485241, score: 0.487879, loss: 1.176242\n",
      "epoch: 51, train loss: 0.629308, score: 0.384848, loss: 1.202565\n",
      "epoch: 52, train loss: 0.482514, score: 0.310606, loss: 1.473943\n",
      "epoch: 53, train loss: 0.511843, score: 0.328788, loss: 1.484240\n",
      "epoch: 54, train loss: 0.465270, score: 0.339394, loss: 1.429495\n",
      "epoch: 55, train loss: 0.903741, score: 0.431818, loss: 1.228243\n",
      "epoch: 56, train loss: 0.897336, score: 0.371212, loss: 1.347589\n",
      "epoch: 57, train loss: 0.308145, score: 0.325758, loss: 1.378675\n",
      "epoch: 58, train loss: 0.499328, score: 0.292424, loss: 1.521250\n",
      "epoch: 59, train loss: 0.532102, score: 0.289394, loss: 1.671608\n",
      "epoch: 60, train loss: 0.275275, score: 0.327273, loss: 1.501450\n",
      "epoch: 61, train loss: 0.225895, score: 0.303030, loss: 1.621173\n",
      "epoch: 62, train loss: 0.117788, score: 0.304545, loss: 1.571064\n",
      "epoch: 63, train loss: 0.565993, score: 0.334848, loss: 1.487910\n",
      "epoch: 64, train loss: 0.218051, score: 0.325758, loss: 1.418355\n",
      "epoch: 65, train loss: 2.640770, score: 0.292424, loss: 1.626633\n",
      "epoch: 66, train loss: 0.357968, score: 0.459091, loss: 1.304867\n",
      "epoch: 67, train loss: 0.826955, score: 0.442424, loss: 1.501469\n",
      "epoch: 68, train loss: 0.780151, score: 0.330303, loss: 1.396433\n",
      "epoch: 69, train loss: 0.884327, score: 0.348485, loss: 1.466281\n",
      "epoch: 70, train loss: 0.596448, score: 0.350000, loss: 1.783798\n",
      "epoch: 71, train loss: 1.138611, score: 0.328788, loss: 1.541228\n",
      "epoch: 72, train loss: 0.213822, score: 0.289394, loss: 1.661837\n",
      "epoch: 73, train loss: 1.476163, score: 0.310606, loss: 1.881627\n",
      "epoch: 74, train loss: 0.373716, score: 0.339394, loss: 1.637079\n",
      "epoch: 75, train loss: 0.252535, score: 0.380303, loss: 1.648452\n",
      "epoch: 76, train loss: 0.409454, score: 0.271212, loss: 6.240017\n",
      "epoch: 77, train loss: 0.542506, score: 0.275758, loss: 2.630659\n",
      "epoch: 78, train loss: 0.684883, score: 0.419697, loss: 2.831930\n",
      "epoch: 79, train loss: 1.231790, score: 0.298485, loss: 2.316453\n",
      "epoch: 80, train loss: 0.973901, score: 0.318182, loss: 1.785206\n",
      "epoch: 81, train loss: 0.699969, score: 0.293939, loss: 1.488798\n",
      "epoch: 82, train loss: 1.085831, score: 0.295455, loss: 1.889821\n",
      "epoch: 83, train loss: 0.315318, score: 0.277273, loss: 1.634858\n",
      "epoch: 84, train loss: 0.193567, score: 0.313636, loss: 1.669641\n",
      "epoch: 85, train loss: 1.046824, score: 0.304545, loss: 1.550146\n",
      "epoch: 86, train loss: 0.098305, score: 0.301515, loss: 2.016836\n",
      "epoch: 87, train loss: 0.640933, score: 0.309091, loss: 1.795873\n",
      "epoch: 88, train loss: 0.329160, score: 0.295455, loss: 1.578536\n",
      "epoch: 89, train loss: 0.789091, score: 0.307576, loss: 1.650941\n",
      "epoch: 90, train loss: 1.418088, score: 0.310606, loss: 1.699520\n",
      "epoch: 91, train loss: 0.601307, score: 0.295455, loss: 1.817739\n",
      "epoch: 92, train loss: 0.645878, score: 0.313636, loss: 1.545632\n",
      "epoch: 93, train loss: 0.587518, score: 0.303030, loss: 1.497915\n",
      "epoch: 94, train loss: 0.232238, score: 0.290909, loss: 1.805719\n",
      "epoch: 95, train loss: 0.247521, score: 0.309091, loss: 1.987633\n",
      "epoch: 96, train loss: 0.339453, score: 0.310606, loss: 1.676388\n",
      "epoch: 97, train loss: 0.414718, score: 0.309091, loss: 1.770316\n",
      "epoch: 98, train loss: 0.320252, score: 0.307576, loss: 1.932097\n",
      "epoch: 99, train loss: 0.510557, score: 0.295455, loss: 2.070108\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(0)\n",
    "# tf.random.set_random_seed(0)\n",
    "\n",
    "frozen_classifier = DNN_Classifier_Frozen(\"Mnist-0to4-best_batch_norm\", use_hidden_layers=3)\n",
    "frozen_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
