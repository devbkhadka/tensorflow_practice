{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new DNN that reuses all the pre-trained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a fresh new one. \n",
    "- Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? \n",
    "- Try caching the frozen layers, and train the model again: how much faster is it now? \n",
    "- Try again reusing just four hidden layers instead of five. Can you achieve a higher precision? \n",
    "- Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "\n",
    "train_indxes_5to9 = y_train>=5\n",
    "train_x_5to9 = x_train[train_indxes_5to9]\n",
    "train_y_5to9 = y_train[train_indxes_5to9]-5\n",
    "\n",
    "\n",
    "def get_random_indxs(y_train, group_value, count=10):\n",
    "    train_indxs = np.arange(len(y_train), dtype=np.int32)\n",
    "    group_indxs = train_indxs[y_train == group_value]\n",
    "    indxs = np.random.choice(group_indxs,count)\n",
    "    return indxs\n",
    "\n",
    "def get_samples_of_each_group(y_train, count=150):\n",
    "    rand_indxs = np.array([], dtype=np.int32)\n",
    "    for group_val in np.unique(y_train):\n",
    "        rand_indxs=np.r_[rand_indxs,get_random_indxs(y_train,group_val, count)]\n",
    "        \n",
    "    np.random.shuffle(rand_indxs)\n",
    "    return rand_indxs\n",
    "\n",
    "rand_indxs = get_samples_of_each_group(train_y_5to9)\n",
    "train_x_5to9 = train_x_5to9[rand_indxs]\n",
    "train_y_5to9 = train_y_5to9[rand_indxs]\n",
    "\n",
    "\n",
    "train_x_5to9, val_x_5to9, train_y_5to9, val_y_5to9 = \\\n",
    "    train_test_split(train_x_5to9, train_y_5to9, test_size=.3, stratify=train_y_5to9)\n",
    "\n",
    "val_x_5to9, test_x_5to9, val_y_5to9, test_y_5to9 = \\\n",
    "    train_test_split(val_x_5to9,val_y_5to9, test_size=.6, stratify=val_y_5to9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that DNN_Classifier is not broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.900000, loss 0.117199\n",
      "epoch 20, score 0.955556, loss 0.001046\n",
      "epoch 40, score 0.933333, loss 0.002180\n",
      "epoch 60, score 0.900000, loss 0.014893\n",
      "epoch 80, score 0.955556, loss 0.013935\n",
      "epoch 100, score 0.922222, loss 0.000119\n",
      "epoch 120, score 0.933333, loss 0.000265\n",
      "epoch 140, score 0.933333, loss 0.000468\n",
      "epoch 160, score 0.966667, loss 0.000082\n",
      "epoch 180, score 0.944444, loss 0.000013\n",
      "epoch 200, score 0.944444, loss 0.000055\n",
      "epoch 220, score 0.966667, loss 0.000005\n",
      "epoch 240, score 0.922222, loss 0.000300\n",
      "epoch 260, score 0.922222, loss 0.000110\n",
      "No progress for 200 epoches.\n",
      "Reverting back to epoch 73                     with 0.977778 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier(activation=<function elu at 0x1a24f7d268>,\n",
       "        batch_norm_momentum=0.95, batch_size=50, dropout_rate=None,\n",
       "        learning_rate=0.01, n_hidden_layers=3, n_neurons=100, n_outputs=5,\n",
       "        optimizer=<class 'tensorflow.python.training.adam.AdamOptimizer'>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from imp import reload\n",
    "# import my_libs\n",
    "# reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "classifier = DNN_Classifier(n_hidden_layers=3, n_neurons=100,n_outputs=5,\n",
    "                            batch_norm_momentum=.95)\n",
    "\n",
    "classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DNN_Classifier_Transfer\n",
    "which will transfer learning of DNN_Classifier from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 19:36:31.123486 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:73: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from imp import reload\n",
    "# import my_libs\n",
    "# reload(my_libs.dnn)\n",
    "\n",
    "import tensorflow as tf\n",
    "from my_libs.dnn import DNN_Classifier, get_optimizer_op, get_validation_score, \\\n",
    "    get_softmax_xentropy_loss\n",
    "from tensorflow.train import AdamOptimizer\n",
    "\n",
    "class DNN_Classifier_Transfer(DNN_Classifier):\n",
    "    def __init__(self, checkpoint_name, use_hidden_layers=0):\n",
    "        DNN_Classifier.__init__(self, batch_size=20)\n",
    "        self._checkpoint_name = checkpoint_name\n",
    "        self._batch_norm_update_ops = None\n",
    "        self.use_hidden_layers = use_hidden_layers\n",
    "        \n",
    "    \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier._restore_graph(self,self._checkpoint_name)\n",
    "        \n",
    "        self.restore_n_hidden_layers()\n",
    "        \n",
    "        get_layer_name = lambda i: \"hidden%d\"%(self.n_hidden_layers-i)\n",
    "        variable_scopes = [get_layer_name(i) for i in range(self.use_hidden_layers)]\n",
    "        \n",
    "        trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"output\")\n",
    "        \n",
    "        for scope in variable_scopes:\n",
    "            trainable_variables.append(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=scope))\n",
    "        \n",
    "        optimizer = AdamOptimizer(0.01, name=\"adam2\")\n",
    "        self._optimizer_op = optimizer.minimize(self._loss, var_list=trainable_variables)\n",
    "        \n",
    "        DNN_Classifier._restore_session(self, self._checkpoint_name)\n",
    "        return self._graph\n",
    "    \n",
    "    def restore_n_hidden_layers(self):\n",
    "        i=1\n",
    "        tensors = []\n",
    "        while True:\n",
    "            try:       \n",
    "                tensor = self._graph.get_tensor_by_name(\"DNN/hiden%d_out:0\"%i)\n",
    "                tensors.append(tensor)\n",
    "                i+=1\n",
    "            except:\n",
    "                break\n",
    "        self.n_hidden_layers = i-1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 19:36:48.534260 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:223: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0827 19:36:48.538044 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:224: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "W0827 19:36:49.280345 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:225: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0827 19:36:49.343497 4489565632 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0827 19:36:49.854130 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:240: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0827 19:36:49.856662 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:242: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0827 19:36:50.104504 4489565632 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0827 19:36:51.452907 4489565632 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:118: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.311111, loss 16.103727\n",
      "epoch 20, score 0.488889, loss 1.239170\n",
      "epoch 40, score 0.466667, loss 1.082394\n",
      "epoch 60, score 0.444444, loss 1.059728\n",
      "epoch 80, score 0.477778, loss 1.024295\n",
      "epoch 100, score 0.488889, loss 1.236668\n",
      "epoch 120, score 0.511111, loss 0.656108\n",
      "epoch 140, score 0.500000, loss 0.786920\n",
      "epoch 160, score 0.555556, loss 0.781088\n",
      "epoch 180, score 0.488889, loss 0.497093\n",
      "epoch 200, score 0.433333, loss 0.188531\n",
      "epoch 220, score 0.377778, loss 0.859469\n",
      "epoch 240, score 0.277778, loss 0.543417\n",
      "epoch 260, score 0.266667, loss 0.434174\n",
      "epoch 280, score 0.266667, loss 0.394407\n",
      "epoch 300, score 0.288889, loss 0.173341\n",
      "epoch 320, score 0.211111, loss 0.562680\n",
      "epoch 340, score 0.266667, loss 0.412937\n",
      "No progress for 200 epoches.\n",
      "Reverting back to epoch 156                     with 0.588889 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier_Transfer(checkpoint_name=None, use_hidden_layers=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(5)\n",
    "# tf.random.set_random_seed(5)\n",
    "\n",
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\", use_hidden_layers=2)\n",
    "transfer_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crate DNN_Classifier_Frozen\n",
    "- Same as DNN_Classifier_Transfer but it will pre calculate output of previous layer for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import my_libs\n",
    "reload(my_libs.dnn)\n",
    "from my_libs.dnn import DNN_Classifier\n",
    "\n",
    "class DNN_Classifier_Frozen(DNN_Classifier_Transfer):\n",
    "        \n",
    "    def _initialize_session_and_graph(self):\n",
    "        DNN_Classifier_Transfer._initialize_session_and_graph(self)\n",
    "        forzen_output_name = \"DNN/hiden%d_out:0\"%(self.n_hidden_layers-self.use_hidden_layers)\n",
    "        \n",
    "        self._frozen_out = self._graph.get_tensor_by_name(forzen_output_name)\n",
    "        ## replace input placeholder _x with output of hidden layer _frozen_out\n",
    "        self._old_x = self._x\n",
    "        self._x = self._frozen_out\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y):\n",
    "        self._initialize_session_and_graph()\n",
    "        with self._session.as_default() as sess:\n",
    "            self._x_frozen_out = self._frozen_out.eval(session=sess,feed_dict={self._old_x:x})\n",
    "            self._val_x_frozen_out = self._frozen_out.eval(feed_dict={self._old_x:val_x})\n",
    "            \n",
    "        return DNN_Classifier.fit(self, self._x_frozen_out, y, self._val_x_frozen_out, val_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, score 0.422222, loss 1.747186\n",
      "epoch 50, score 0.233333, loss 0.270999\n",
      "epoch 100, score 0.133333, loss 0.422296\n",
      "epoch 150, score 0.200000, loss 0.510700\n",
      "epoch 200, score 0.211111, loss 0.245352\n",
      "No progress for 200 epoches.\n",
      "Reverting back to epoch 0                     with 0.422222 score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNN_Classifier_Frozen(checkpoint_name=None, use_hidden_layers=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_random_seed(0)\n",
    "\n",
    "frozen_classifier = DNN_Classifier_Frozen(\"Mnist-0to4-best_batch_norm\", use_hidden_layers=2)\n",
    "frozen_classifier.fit(train_x_5to9, train_y_5to9, val_x_5to9, val_y_5to9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if running variable initializer resets previously set value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "xx = tf.Variable(0, dtype=np.int16, name=\"xx\")\n",
    "inc = tf.assign(xx, xx+1)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run(inc)\n",
    "    sess.run(init)\n",
    "    print(xx.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 09:49:44.197348 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:223: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0827 09:49:44.199877 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:224: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "W0827 09:49:45.264413 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:225: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0827 09:49:45.501465 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:240: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0827 09:49:45.504031 4536214976 deprecation_wrapper.py:119] From /Volumes/Projects/Machine Learning/tensorflow_practice/my_libs/dnn.py:242: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0827 09:49:45.923485 4536214976 deprecation.py:323] From /Users/devbhadurkhadka/.pyenv/versions/anaconda3-5.2.0/envs/scikit_practice/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "transfer_classifier = DNN_Classifier_Transfer(\"Mnist-0to4-best_batch_norm\")\n",
    "graph=transfer_classifier._initialize_session_and_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DNN/hiden1_out',\n",
       " 'DNN/hiden2_out',\n",
       " 'DNN/hiden3_out',\n",
       " 'DNN/hiden4_out',\n",
       " 'DNN/hiden5_out']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[op.name for op in graph.get_operations() if ptrn.search(op.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(4, 14), match='hiden5_out'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ptrn=re.compile(r\"hiden\\d+_out$\")\n",
    "print(ptrn.search(\"DNN/hiden5_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_hidden_layers(graph):\n",
    "    i=1\n",
    "    tensors = []\n",
    "    while True:\n",
    "        try:       \n",
    "            tensor = graph.get_tensor_by_name(\"DNN/hiden%d_out:0\"%i)\n",
    "            tensors.append(tensor)\n",
    "            i+=1\n",
    "        except:\n",
    "            break\n",
    "    return i-1\n",
    "\n",
    "get_n_hidden_layers(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden5/kernel:0' shape=(150, 150) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/bias:0' shape=(150,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                scope=\"hidden5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
